### Задача 1.
Напишите список операций, которые вы будете производить для остановки запроса пользователя
- Использовать `db.currentOp({...})` с необходимыми фильтрациями для нахождения `operation ID`
- Возможно, длинное выполнение операции было вызвано блокировкой другим запросом
- Использую `db.killOp(...)`

Предложите вариант решения проблемы с долгими (зависающими) запросами в MongoDB
- Мониторинг и алертинг для обнаружения медленных запросов
- `db.setProfilingLevel(1)` и настройка `slowms` для логирования медленных запросов и оптимизации в дальнейшем

### Задача 2.
При масштабировании сервиса до N реплик вы увидели, что:
- сначала рост отношения записанных значений к истекшим
- Redis блокирует операции записи

Как вы думаете, в чем может быть проблема?
- Проблема может быть связана с достижением `maxmemory`. Если также `maxmemory-policy=noeviction`, то Redis будет 
  выдавать ошибки на операции записи по типу: `SET, LPUSH, and so on` и будет работать только на операции чтения.
- Возможно, из-за дефолтной настройки параметра `ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP=20`, который позволяет "активно" 
  "просрочить" в секунду максимум 200 ключей. И "ленивой" "просрочки" ключей не достаточно.

Комментарий от преподавателя:
```markdown
Redis выполняет очистку записей с TTL по 20 штук в 100 мс. То есть в секунду он чистит 200 записей.
Обычно этого хватает.
Но если записей для очистки в 1 секунду становится больше 200 - Redis начинает блокировать работу с ним, пока
все не подчистит.
Как решение - возможно просто поднять 2 инстанса Redis и на уровне приложения переключаться между ними.
```

### Задача 3.
Как вы думаете, почему это начало происходить и как локализовать проблему?
- Похоже, пользователи стали запрашивать большое количество записей.
- Для локализации попрошу предоставить запрос, посмотрю на источник, типы данных, количества

Какие пути решения данной проблемы вы можете предложить?
- Можно увеличить `net_read_timeout`
- Возможно, проблему провоцируют такие типы данных как `TEXT` или `BLOB` и может помочь увеличение параметра `max_allowed_packet`
- Возможно, эффективнее будет выносить данные по запросу, например, по расписанию, в какое-либо хранилище для 
  последующей обработки, например, в `s3`. (нужно понять зачем эти данные нужны и кому и когда :)

### Задача 4.
Как вы думаете, что происходит?
- Похоже, `Postgres` использует слишком много оперативной памяти и `OOM killer` убивает процесс с базой данных.

Как бы вы решили данную проблему?
- В зависимости от ситуации настроил бы или отключил бы `OOM killer`
- Добавил мощностей хосту с БД

Комментарий от преподавателя:
```markdown
Расширять сервер вертикально можно, но всегда будет ситуация, что уже невозможно 
будет накинуть ресурсов безболезненно. Предлагаю настроить OOM-killer так, чтобы 
родительский процесс не убивался, а дочерние - свободно. Если убивать родительский 
процесс, то все дочерние тоже будут завершены, а это не хорошо, так как некоторые
дочерние процессы из всей пачки процессов постгри могут и не превышать лимита памяти. 
https://www.postgresql.org/docs/current/kernel-resources.html#LINUX-MEMORY-OVERCOMMIT- тут подробнее описано.
```